https://blog.csdn.net/qq_30984075/article/details/51693405?locationNum=12
ML组件

ML的标准API使用管道（pipeline）这样的方式，可以将多个算法或者数据处理过程整合到一个管道或者一个流程里运行，其中包含下面几个部分：
1. dataFrame：用于ML的dataset，保存数据
2. transformer：将一个dataFrame按照某种计算转换成另外一个dataFrame，例如把一个包含特征的dataFrame通过模型预测，生成一个包含特征和预测的dataFrame
3. estimator：根据训练样本进行模型训练（fit），并且得到一个对应的transformer
4. pipeline：将多个transformer和estimator串成一个ML的工作流
5. parameter：transformer和estimator共用一套API来确定参数

    spark的机器学习包有两个，一个是ML，一个是MLlib，前者是基于dataFrame的API实现的，后者是基于RDD的API实现的，官网推荐用前者，使用比较方便。

transformer

一个transformer包含特征转换和已学习得到的数据模型，它实现了一个方法transform()
例如：一个特征transformer可能将一个dataFrame的某些列映射成新的列，然后输出处理后的新的dataFrame；一个学习得到的模型将读取一个包含特征的dataFrame，对每个样本进行预测，并且把预测结果附加到这个dataFrame，得到一个新的dataFrame
Estimators

主要用于训练模型，实现了一个方法fit()，接受一个包含特征的dataFrame，然后训练得到一个模型，那个模型就是一个transformer
例如：一个LogisticRegression是一个estimator，然后通过调用fit()，得到一个LogisticRegressionModel，这是一个transformer。

    每个transformer和estimator都有一个唯一ID，用于保存对应的参数

pipeline

例如一个文本挖掘包含以下三个步骤：
1. 将文本切分成词
2. 将词转换成特征向量
3. 训练得到一个模型，然后用于预测

spark ML将这样一个工作流定义为pipeline，一个pipeline包含多个PipelineStages (transformer和estimator)，通过dataFrame在各个stage中进行传递。

image

这是一个训练模型的例子，包含了三个步骤，蓝色的是指transformer，红色是estimator

image
这是一个使用已训练模型预测样本的例子，
Parameters

一个Paramap包含多个(parameter, value)的键值对
有两种方法将参数传给算法：
1. 将参数设置到算法的一个实例，例如lr是LogisticRegression的一个实例，则他可以调用lr.setMaxIter(10)来设置训练循环次数
2. 将paramap作为输入参数，给fit()或者transform()，这些参数会都会覆盖掉原来set的值

    我们可以将paramap传给不同实例，例如lr1和lr2是LogisticRegression的两个实例，我们可以建立ParamMap(lr1.maxIter -> 10, lr2.maxIter -> 20)的参数列表，即将两个实例的参数都放在paramMap中

    spark1.6的版本可以使用import/export导出模型或者pipeline到磁盘上

范例1

import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.mllib.linalg.{Vector, Vectors}
import org.apache.spark.sql.Row

val training = sqlContext.createDataFrame(Seq(
  (1.0, Vectors.dense(0.0, 1.1, 0.1)),
  (0.0, Vectors.dense(2.0, 1.0, -1.0)),
  (0.0, Vectors.dense(2.0, 1.3, 1.0)),
  (1.0, Vectors.dense(0.0, 1.2, -0.5))
)).toDF("label", "features")

//创建一个LogisticRegression实例，这是一个Estimator.
val lr = new LogisticRegression()
//打印参数
println("LogisticRegression parameters:\n" + lr.explainParams() + "\n")

//调用实例的set方法设置参数
lr.setMaxIter(10)
  .setRegParam(0.01)

// 学习LogisticRegression模型，model1是一个transformer
val model1 = lr.fit(training)

println("Model 1 was fit using parameters: " + model1.parent.extractParamMap)

// 通过paramap来设置参数
val paramMap = ParamMap(lr.maxIter -> 20)
  .put(lr.maxIter, 30) 
  .put(lr.regParam -> 0.1, lr.threshold -> 0.55)

// 两个ParamMap之间可以相加合并.
val paramMap2 = ParamMap(lr.probabilityCol -> "myProbability") // Change output column name
val paramMapCombined = paramMap ++ paramMap2

val model2 = lr.fit(training, paramMapCombined)
println("Model 2 was fit using parameters: " + model2.parent.extractParamMap)

//测试数据
val test = sqlContext.createDataFrame(Seq(
  (1.0, Vectors.dense(-1.0, 1.5, 1.3)),
  (0.0, Vectors.dense(3.0, 2.0, -0.1)),
  (1.0, Vectors.dense(0.0, 2.2, -1.5))
)).toDF("label", "features")

//model2的transform()会只选择features的数据，不会把label数据包含进去
model2.transform(test)
  .select("features", "label", "myProbability", "prediction")
  .collect()
  .foreach { case Row(features: Vector, label: Double, prob: Vector, prediction: Double) =>
    println(s"($features, $label) -> prob=$prob, prediction=$prediction")
  }

    1
    2
    3
    4
    5
    6
    7
    8
    9
    10
    11
    12
    13
    14
    15
    16
    17
    18
    19
    20
    21
    22
    23
    24
    25
    26
    27
    28
    29
    30
    31
    32
    33
    34
    35
    36
    37
    38
    39
    40
    41
    42
    43
    44
    45
    46
    47
    48
    49
    50
    51
    52

import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.feature.{HashingTF, Tokenizer}
import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.sql.Row

val training = sqlContext.createDataFrame(Seq(
  (0L, "a b c d e spark", 1.0),
  (1L, "b d", 0.0),
  (2L, "spark f g h", 1.0),
  (3L, "hadoop mapreduce", 0.0)
)).toDF("id", "text", "label")

/* 初始化一个pipeline，包含三个步骤: tokenizer, hashingTF, and lr.
tokenizer 负责切词，hashingTF负责按词进行特征排列，lr负责模型训练 */
val tokenizer = new Tokenizer()
  .setInputCol("text")
  .setOutputCol("words")
val hashingTF = new HashingTF()
  .setNumFeatures(1000)
  .setInputCol(tokenizer.getOutputCol)
  .setOutputCol("features")
val lr = new LogisticRegression()
  .setMaxIter(10)
  .setRegParam(0.01)
val pipeline = new Pipeline()
  .setStages(Array(tokenizer, hashingTF, lr))

val model = pipeline.fit(training)

// 将训练后得到的模型保存到磁盘
model.save("/tmp/spark-logistic-regression-model")

// 把未训练的pipeline保存到磁盘
pipeline.save("/tmp/unfit-lr-model")

// 从磁盘读取模型
val sameModel = PipelineModel.load("/tmp/spark-logistic-regression-model")

// 测试数据
val test = sqlContext.createDataFrame(Seq(
  (4L, "spark i j k"),
  (5L, "l m n"),
  (6L, "mapreduce spark"),
  (7L, "apache hadoop")
)).toDF("id", "text")

model.transform(test)
  .select("id", "text", "probability", "prediction")
  .collect()
  .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =>
    println(s"($id, $text) --> prob=$prob, prediction=$prediction")
  }

    1
    2
    3
    4
    5
    6
    7
    8
    9
    10
    11
    12
    13
    14
    15
    16
    17
    18
    19
    20
    21
    22
    23
    24
    25
    26
    27
    28
    29
    30
    31
    32
    33
    34
    35
    36
    37
    38
    39
    40
    41
    42
    43
    44
    45
    46
    47
    48
    49
    50
    51
    52
    53

model selection

    ML里面用CrossValidator类来做交叉验证，这个类包含一个estimator、一堆paramMap、和一个evaluator。
    evaluator有三个子类，包括regressionEvaluator, BinaryClassificationEvaluator, MulticlassClassificationEvaluator。

import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
import org.apache.spark.ml.feature.{HashingTF, Tokenizer}
import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}
import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.sql.Row

val training = sqlContext.createDataFrame(Seq(
  (0L, "a b c d e spark", 1.0),
  (1L, "b d", 0.0),
  (2L, "spark f g h", 1.0),
  (3L, "hadoop mapreduce", 0.0),
  (4L, "b spark who", 1.0),
  (5L, "g d a y", 0.0),
  (6L, "spark fly", 1.0),
  (7L, "was mapreduce", 0.0),
  (8L, "e spark program", 1.0),
  (9L, "a e c l", 0.0),
  (10L, "spark compile", 1.0),
  (11L, "hadoop software", 0.0)
)).toDF("id", "text", "label")

val tokenizer = new Tokenizer()
  .setInputCol("text")
  .setOutputCol("words")
val hashingTF = new HashingTF()
  .setInputCol(tokenizer.getOutputCol)
  .setOutputCol("features")
val lr = new LogisticRegression()
  .setMaxIter(10)
val pipeline = new Pipeline()
  .setStages(Array(tokenizer, hashingTF, lr))

// ParamGridBuilder创建参数grid，保存所有要做验证的参数
val paramGrid = new ParamGridBuilder()
  .addGrid(hashingTF.numFeatures, Array(10, 100, 1000))
  .addGrid(lr.regParam, Array(0.1, 0.01))
  .build()

// 这里将pipeline作为一个estimator传递给cv，这里默认的评估是ROC
val cv = new CrossValidator()
  .setEstimator(pipeline)
  .setEvaluator(new BinaryClassificationEvaluator)
  .setEstimatorParamMaps(paramGrid)
  .setNumFolds(2) // Use 3+ in practice

// 训练模型，选择最优参数
val cvModel = cv.fit(training)

val test = sqlContext.createDataFrame(Seq(
  (4L, "spark i j k"),
  (5L, "l m n"),
  (6L, "mapreduce spark"),
  (7L, "apache hadoop")
)).toDF("id", "text")

// cvModel将会用最优的参数进行预测
cvModel.transform(test)
  .select("id", "text", "probability", "prediction")
  .collect()
  .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =>
    println(s"($id, $text) --> prob=$prob, prediction=$prediction")
  }

    1
    2
    3
    4
    5
    6
    7
    8
    9
    10
    11
    12
    13
    14
    15
    16
    17
    18
    19
    20
    21
    22
    23
    24
    25
    26
    27
    28
    29
    30
    31
    32
    33
    34
    35
    36
    37
    38
    39
    40
    41
    42
    43
    44
    45
    46
    47
    48
    49
    50
    51
    52
    53
    54
    55
    56
    57
    58
    59
    60
    61
    62
    63
    64

    ML中除了cv以外，还有一种指定样本划分的验证方式，TrainValidationSplit 类，默认是0.75，即3/4用于做训练，1/4用于做测试。其他跟cv一样

import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}

val data = sqlContext.read.format("libsvm").load("data/mllib/sample_linear_regression_data.txt")
val Array(training, test) = data.randomSplit(Array(0.9, 0.1), seed = 12345)

val lr = new LinearRegression()

val paramGrid = new ParamGridBuilder()
  .addGrid(lr.regParam, Array(0.1, 0.01))
  .addGrid(lr.fitIntercept)
  .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))
  .build()

// 创建trainValidationSplit类
val trainValidationSplit = new TrainValidationSplit()
  .setEstimator(lr)
  .setEvaluator(new RegressionEvaluator)
  .setEstimatorParamMaps(paramGrid)
  // 指定划分百分比
  .setTrainRatio(0.8)


val model = trainValidationSplit.fit(training)

model.transform(test)
  .select("features", "label", "prediction")
  .show()
